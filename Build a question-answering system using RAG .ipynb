{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "openai.api_key = \"enter your open ai key\"\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def build_vector_index(chunks):\n",
    "    embeddings = embedding_model.encode(chunks)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index, embeddings\n",
    "\n",
    "def retrieve_relevant_chunks(query, chunks, index, embeddings, top_k=3):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    prompt = f\"Answer the question based on the context below:\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",  # You can use 'gpt-3.5-turbo' for chat-based models\n",
    "        prompt=prompt,\n",
    "        max_tokens=200,\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def main(file_path, query):\n",
    "    # Step 1: Extract and preprocess document\n",
    "    text = extract_text_from_pdf(file_path)\n",
    "    chunks = chunk_text(text)\n",
    "\n",
    "    # Step 2: Build vector index\n",
    "    index, embeddings = build_vector_index(chunks)\n",
    "\n",
    "    # Step 3: Retrieve relevant chunks\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, chunks, index, embeddings)\n",
    "    context = \" \".join(relevant_chunks)\n",
    "\n",
    "    # Step 4: Generate answer\n",
    "    answer = generate_answer(query, context)\n",
    "    return answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    document_path = \" \"\n",
    "\n",
    "    user_query = \"What are the benefits of using LoRA in fine-tuning?\"\n",
    "\n",
    "    answer = main(document_path, user_query)\n",
    "    print(\"Answer:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
